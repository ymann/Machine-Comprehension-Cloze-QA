{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "base-model-700.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ymann/Machine-Comprehension-Cloze-QA/blob/master/base_model_700.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "CNS4OZXxMZ-A",
        "outputId": "770442b9-6d72-484f-ef8c-53c51cb7a015",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        }
      },
      "cell_type": "code",
      "source": [
        "#Download and unzip files\n",
        "!pip3 install scikit-learn\n",
        "!pip3 install gensim"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: scipy>=0.13.3 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.2.1)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.12.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.2.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.8.2)\n",
            "Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.49.0)\n",
            "Requirement already satisfied: bz2file in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (0.98)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.21.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (1.9.134)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.24.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2019.3.9)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.134 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (1.12.134)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.2.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.134->boto3->smart-open>=1.2.1->gensim) (2.5.3)\n",
            "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.134->boto3->smart-open>=1.2.1->gensim) (0.14)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RpUkUwIf0RQU",
        "colab_type": "code",
        "outputId": "0e9f17b5-7f4f-4e03-f622-bda16a28603f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "cell_type": "code",
      "source": [
        "! sudo apt-get install unzip\n",
        "! wget http://seas.upenn.edu/~branlin/cnn.tgz \n",
        "! tar -xzf cnn.tgz\n",
        "# ! wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "# ! unzip glove.6B.zip\n",
        "# ! touch word2vec.txt\n",
        "  \n",
        "  \n",
        "# Mount your google drive. \n",
        "# Use this to save your PyTorch model for submission\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')\n",
        "# !mkdir /content/gdrive/My\\ Drive/gloves\n",
        "# #Test drive access. \n",
        "# #You should have a test.txt under a new folder cis530_hw6 in your Google drive\n",
        "# with open('/content/gdrive/My Drive/gloves/', 'w') as f:\n",
        "#   f.write('This is a test!')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "unzip is already the newest version (6.0-21ubuntu1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-410\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 6 not upgraded.\n",
            "--2019-04-27 19:30:50--  http://seas.upenn.edu/~branlin/cnn.tgz\n",
            "Resolving seas.upenn.edu (seas.upenn.edu)... 158.130.68.91\n",
            "Connecting to seas.upenn.edu (seas.upenn.edu)|158.130.68.91|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://www.seas.upenn.edu/~branlin/cnn.tgz [following]\n",
            "--2019-04-27 19:30:55--  https://www.seas.upenn.edu/~branlin/cnn.tgz\n",
            "Resolving www.seas.upenn.edu (www.seas.upenn.edu)... 158.130.68.91, 2607:f470:8:64:5ea5::9\n",
            "Connecting to www.seas.upenn.edu (www.seas.upenn.edu)|158.130.68.91|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 217447312 (207M) [application/x-gzip]\n",
            "Saving to: ‘cnn.tgz’\n",
            "\n",
            "cnn.tgz             100%[===================>] 207.37M  41.1MB/s    in 5.5s    \n",
            "\n",
            "2019-04-27 19:31:01 (37.6 MB/s) - ‘cnn.tgz’ saved [217447312/217447312]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Eb_DrywYHsea",
        "colab_type": "code",
        "outputId": "8278a68b-b832-4c16-fdc6-4286e10dee21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "cell_type": "code",
      "source": [
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "!pip3 install https://download.pytorch.org/whl/cu100/torch-1.0.1-cp36-cp36m-linux_x86_64.whl\n",
        "!pip3 install torch torchvision\n",
        "  \n",
        "import torch\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# device = 'cpu'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch==1.0.1 from https://download.pytorch.org/whl/cu100/torch-1.0.1-cp36-cp36m-linux_x86_64.whl\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu100/torch-1.0.1-cp36-cp36m-linux_x86_64.whl (614.8MB)\n",
            "\u001b[K    100% |████████████████████████████████| 614.8MB 24kB/s \n",
            "\u001b[?25hInstalling collected packages: torch\n",
            "  Found existing installation: torch 1.0.1.post2\n",
            "    Uninstalling torch-1.0.1.post2:\n",
            "      Successfully uninstalled torch-1.0.1.post2\n",
            "Successfully installed torch-1.0.1\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.0.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.2.2.post3)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (4.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.16.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.12.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision) (0.46)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MEKp_8FConPP",
        "colab_type": "code",
        "outputId": "8509b1fb-f84a-4b6a-9edf-a93b9ed09d3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/cis700/hw1-release.git\n",
        "!mv hw1-release/helper.py .\n",
        "!rm -rf hw1-release/\n",
        "!rm -rf hw1/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'hw1-release'...\n",
            "remote: Enumerating objects: 26, done.\u001b[K\n",
            "remote: Counting objects: 100% (26/26), done.\u001b[K\n",
            "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
            "remote: Total 26 (delta 6), reused 23 (delta 6), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (26/26), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vlW8S1bMoaRP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "LOG_DIR = './logs'\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")\n",
        "\n",
        "!if [ -f ngrok ] ; then echo \"Ngrok already installed\" ; else wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip > /dev/null 2>&1 && unzip ngrok-stable-linux-amd64.zip > /dev/null 2>&1 ; fi\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h68zP3nhohEM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "get_ipython().system_raw('./ngrok http 6006 &')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LS3Mgx2Hojq4",
        "colab_type": "code",
        "outputId": "f49a255d-b630-468d-b6ef-f98d6f276e43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print('Tensorboard Link: ' +str(json.load(sys.stdin)['tunnels'][0]['public_url']))\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensorboard Link: https://97c4320e.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jD-g0Ay3LUjv",
        "colab_type": "code",
        "outputId": "2425a5ef-635e-45c2-cc5b-b4d33dd4839e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "from collections import Counter\n",
        "from nltk import ngrams\n",
        "from gensim.test.utils import datapath, get_tmpfile\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "from gensim.models.phrases import Phraser, Phrases\n",
        "from gensim.test.utils import common_texts, get_tmpfile\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec, get_glove_info\n",
        "import numpy as np\n",
        "from torch.nn.utils.rnn import pad_packed_sequence as unpack\n",
        "from torch.nn.utils.rnn import pack_padded_sequence as pack"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
            "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "oFptVk1Pv03f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# %%writefile questions.py\n",
        "import torch\n",
        "import os\n",
        "\n",
        "class QuestionDataset(torch.utils.data.Dataset):\n",
        "  \n",
        "  def __init__(self, root_dir):\n",
        "    self.root_dir = root_dir\n",
        "    self.files = os.listdir(os.path.join(os.getcwd(), root_dir))\n",
        "    \n",
        "  def __len__(self):\n",
        "    return len(self.files)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    if idx % 10000 == 0:\n",
        "      print (idx)\n",
        "    with open(os.path.join(self.root_dir, self.files[idx]), 'r') as f:\n",
        "      s = f.read()\n",
        "    \n",
        "    lines = s.split('\\n')\n",
        "    doc = lines[2]\n",
        "    tokens = doc.split()\n",
        "    query = lines[4]\n",
        "    query_tokens = query.split()\n",
        "    answer = lines[6]\n",
        "#     ent = [tuple(i.split(\":\")) for i in lines[8:]]\n",
        "#     entities = {i:j for i,j in ent}\n",
        "    return (tokens, query_tokens, answer)\n",
        "\n",
        "train_set = QuestionDataset(\"cnn/questions/training\")\n",
        "test_set = QuestionDataset(\"cnn/questions/test\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dZ0Ayd6xwl61",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_set = QuestionDataset(\"cnn/questions/training\")\n",
        "test_set = QuestionDataset(\"cnn/questions/test\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oL3I4KoIRgp3",
        "colab_type": "code",
        "outputId": "fec64e3e-c4f8-4ae6-d759-810aa15c4eab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# model.get_vector('ryan')\n",
        "# get_glove_info(glove_file)\n",
        "len(test_set)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3198"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "rQcBYggu3WWk",
        "colab_type": "code",
        "outputId": "41134a3b-6637-46bc-98f5-80a56404e837",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        }
      },
      "cell_type": "code",
      "source": [
        "# print(train_set[0][0])\n",
        "glove_file = datapath('/content/glove.6B.50d.txt')\n",
        "tmp_file = get_tmpfile(\"./word2vec.txt\")\n",
        "glove2word2vec(glove_file, tmp_file)\n",
        "em = KeyedVectors.load_word2vec_format(tmp_file)  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-99401e9b6a1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mglove_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatapath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/glove.6B.50d.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtmp_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tmpfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./word2vec.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mglove2word2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglove_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmp_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/scripts/glove2word2vec.py\u001b[0m in \u001b[0;36mglove2word2vec\u001b[0;34m(glove_input_file, word2vec_output_file)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \"\"\"\n\u001b[0;32m--> 104\u001b[0;31m     \u001b[0mnum_lines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_glove_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglove_input_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"converting %i vectors from %s to %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_lines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglove_input_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2vec_output_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msmart_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword2vec_output_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/scripts/glove2word2vec.py\u001b[0m in \u001b[0;36mget_glove_info\u001b[0;34m(glove_file_name)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \"\"\"\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0msmart_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglove_file_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0mnum_lines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msmart_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglove_file_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36msmart_open\u001b[0;34m(uri, mode, **kw)\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0mtransport_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_ext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_extension\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransport_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransport_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mscrubbed_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, transport_params)\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m         \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m     )\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, ignore_ext, buffering, encoding, errors)\u001b[0m\n\u001b[1;32m    491\u001b[0m     \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPY3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muri_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopen_kwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muri_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/glove.6B.50d.txt'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "tCDyDuDwyxoL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%writefile BiLSTM.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.utils.rnn as rnn_utils\n",
        "from torch.nn.utils.rnn import pad_packed_sequence as unpack\n",
        "from torch.nn.utils.rnn import pack_padded_sequence as pack\n",
        "\n",
        "class BiLSTM(nn.Module):\n",
        "    def __init__(self, emsize, hidden_dim, vocab_size, num_entities, batch_size, weights_matrix, dropout=0.2):\n",
        "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        super(BiLSTM, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.emsize = emsize\n",
        "        self.batch_size = batch_size\n",
        "        self.hidden_init = (torch.zeros(2, self.batch_size, self.hidden_dim, requires_grad=True).to(device)), (torch.zeros(2, self.batch_size, self.hidden_dim, requires_grad=True).to(device))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.start_symbols = torch.ones(10, 1).long().to(device)\n",
        "        self.fc1 = nn.Linear(hidden_dim*2, num_entities)\n",
        "        # Vocabulary embedding\n",
        "        self.embedding = nn.Embedding(vocab_size, emsize)\n",
        "        self.embedding.load_state_dict({'weight': weights_matrix})\n",
        "        self.embedding.weight.requires_grad = False\n",
        "        # RNN Encoder and Decoder\n",
        "        self.lstm = nn.LSTM(input_size=emsize,\n",
        "                               hidden_size=hidden_dim,\n",
        "                               dropout=dropout,\n",
        "                               batch_first=True,\n",
        "                               bidirectional=True)\n",
        "        \n",
        "    def forward(self, dq):\n",
        "      padded = rnn_utils.pad_sequence(dq).transpose(1, 0)\n",
        "      lengths = torch.Tensor([len(c) for c in dq])\n",
        "      embedded = self.embedding(padded)\n",
        "     #pack sequence\n",
        "      packed_embed = pack(embedded, lengths, batch_first=True)\n",
        "      packed_output, (hidden, cell) = self.lstm(packed_embed)\n",
        "\n",
        "      #unpack sequence\n",
        "      output, input_sizes = unpack(packed_output, batch_first=True)\n",
        "        \n",
        "      final_output = output[torch.arange(lengths.size()[0], out=torch.LongTensor()), lengths.long() - 1]\n",
        "      return self.fc1(final_output)\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9draJ4u-dhIn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# # Computing vocabulary size\n",
        "# words = set()\n",
        "# entities = set()\n",
        "# word_dict = {}\n",
        "# curr_index = 2\n",
        "# for d, q, _ in train_set:\n",
        "#   thing = set(d + q)\n",
        "#   entity_things = {i for i in thing if i.startswith('@entity')}\n",
        "#   words = words.union(thing)\n",
        "#   entities = entities.union(entity_things)\n",
        "#   for i in words:\n",
        "#     if i not in word_dict:\n",
        "#       word_dict[i] = curr_index\n",
        "#       curr_index += 1\n",
        "#   del thing\n",
        "#   del entity_things\n",
        "# for d, q, _ in test_set:\n",
        "#   thing = set(d + q)\n",
        "#   entity_things = {i for i in thing if i.startswith('@entity')}\n",
        "#   words.union(thing)\n",
        "#   entities = entities.union(entity_things)\n",
        "#   for i in words:\n",
        "#     if i not in word_dict:\n",
        "#       word_dict[i] = curr_index\n",
        "#       curr_index += 1\n",
        "#   del thing\n",
        "#   del entity_things\n",
        "# print(len(words))\n",
        "# print(len(entities))\n",
        "\n",
        "with open('vocab.txt', 'a+') as f:\n",
        "  for i, j in word_dict.items():\n",
        "    f.write(i + \"\\t\" + str(j) + \"\\n\")\n",
        "# num_vocab = set().union(*[set(d + q) for d, q, _ in train_set]).size()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r_vMpjWuhABN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for d, q, _ in test_set:\n",
        "  thing = set(d + q)\n",
        "  words.union(*thing)\n",
        "  del thing\n",
        "print(len(words))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A3y6PFOACiNh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_matrix(vocab_size, vocab_file, model, embedding_size=50):\n",
        "    with open(vocab_file, 'r') as f:\n",
        "        s = f.readlines()\n",
        "    vocab_dict = {i.split('\\t')[0]: int(i.split('\\t')[1]) for i in s\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_size))\n",
        "    for i, j in vocab_dict.items():\n",
        "        try:\n",
        "            embedding_matrix[j] = np.array(model.get_vector(i))\n",
        "        except KeyError:\n",
        "            embedding_matrix[j] = np.random.normal(0, 1, embedding_size)\n",
        "    return torch.Tensor(embedding_matrix).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Zho4RwcYBA6Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5Y3H4lztTLN4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "emsize = 50\n",
        "hidden_dim = 75\n",
        "vocab_size = 118497 + 1 # for the delimiter\n",
        "num_entities = 584\n",
        "batch_size = 50\n",
        "num_epochs = 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rj5F7nq8yHOt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%writefile foo.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "from collections import Counter\n",
        "from nltk import ngrams\n",
        "from gensim.test.utils import datapath, get_tmpfile\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "from gensim.models.phrases import Phraser, Phrases\n",
        "from gensim.test.utils import common_texts, get_tmpfile\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec, get_glove_info\n",
        "import numpy as np\n",
        "from torch.nn.utils.rnn import pad_packed_sequence as unpack\n",
        "from torch.nn.utils.rnn import pack_padded_sequence as pack\n",
        "from questions import train_set, test_set\n",
        "from BiLSTM import BiLSTM\n",
        "import datetime\n",
        "from helper import Logger\n",
        "\n",
        "emsize = 50\n",
        "hidden_dim = 75\n",
        "vocab_size = 118497 + 1 # for the delimiter\n",
        "num_entities = 584\n",
        "batch_size = 50\n",
        "num_epochs = 5\n",
        "\n",
        "logger = Logger('./logs/run_' + str(datetime.datetime.now()))\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "weights_matrix = create_matrix(vocab_size, \"vocab.txt\", model)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "model = BiLSTM(emsize, hidden_dim, vocab_size, num_entities, batch_size, weights_matrix).to(device)\n",
        "num_words_indexed = 2\n",
        "num_entities_indexed = 0\n",
        "vocab_dict = {'|||': 1} # word : index\n",
        "entity_dict = dict()\n",
        "\n",
        "def index_strings(strings):\n",
        "  '''strings is a list of strings'''\n",
        "  l = []\n",
        "  global num_words_indexed\n",
        "  global num_entities_indexed\n",
        "  for i in strings:\n",
        "    if i in vocab_dict:\n",
        "      l.append(vocab_dict[i])\n",
        "    else:\n",
        "      vocab_dict[i] = num_words_indexed\n",
        "      num_words_indexed += 1\n",
        "    if i.startswith('@entity'):\n",
        "      if i not in entity_dict:\n",
        "        entity_dict[i] = num_entities_indexed\n",
        "        num_entities_indexed += 1\n",
        "  return l\n",
        "\n",
        "def train_model(model, train_data):\n",
        "  global entities\n",
        "  step = 0\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "  test_loader = DataLoader(train_set, batch_size = batch_size, collate_fn=lambda x : x, shuffle=True)\n",
        "  model = model.to(device)\n",
        "  loss_criterion = criterion.to(device)\n",
        "  for epoch in range(num_epochs):\n",
        "    # (batch_d,batch_q,batch_label)\n",
        "    data_iter = iter(test_loader)\n",
        "    for i, thing in enumerate(data_iter):\n",
        "      sentence_converted = [(index_strings(d), index_strings(q), entity_dict[e]) for d, q, e in thing]\n",
        "      dq = [torch.Tensor(d + [1] + q).long().to(device) for d, q, e in sentence_converted]\n",
        "      lengths = torch.Tensor([len(c) for c in dq])\n",
        "      lengths, idx = lengths.sort(0, descending=True)\n",
        "      dq = [dq[i] for i in idx]\n",
        "      expected = torch.Tensor([e for _, _, e in sentence_converted]).flatten().long().to(device)\n",
        "      expected = expected[idx]\n",
        "      output = model(dq)\n",
        "      \n",
        "      entities = [sorted(list(set([entity_dict[e] for e in d if e.startswith(\"@entity\")]))) for d, _, _ in thing]\n",
        "      entities = [entities[i] for i in idx]\n",
        "      \n",
        "      def calculate_loss(i):\n",
        "        t = output[i]\n",
        "        predictions = t[entities[i]]\n",
        "        index_of_correct = entities[i].index(expected[i].item())\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        return criterion(predictions.unsqueeze(0), torch.Tensor([index_of_correct]).long().to(device))\n",
        "      \n",
        "      loss = sum([calculate_loss(i) for i in range(len(thing))]) / len(thing)\n",
        "      \n",
        "      optimizer.zero_grad()\n",
        "#       loss = loss_criterion(output, expected)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      \n",
        "      logger.scalar_summary(\"loss\", loss.item(), step+1)\n",
        "      step += 1\n",
        "\n",
        "train_model(model, test_set)\n",
        "\n",
        "def test_model(model, test_data):\n",
        "  total = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    test_loader = DataLoader(test_set, batch_size = 1, collate_fn=lambda x : x)\n",
        "    for i, thing in enumerate(test_loader):\n",
        "      sentence_converted = [(index_strings(d), index_strings(q), entity_dict[e]) for d, q, e in thing]\n",
        "      dq = [torch.Tensor(d + [1] + q).long().to(device) for d, q, e in sentence_converted]\n",
        "      lengths = torch.Tensor([len(c) for c in dq])\n",
        "      lengths, idx = lengths.sort(0, descending=True)\n",
        "      dq = [dq[i] for i in idx]\n",
        "      expected = torch.Tensor([e for _, _, e in sentence_converted]).flatten().long().to(device)\n",
        "      expected = expected[idx]\n",
        "      output = model(dq)\n",
        "      \n",
        "      entities = [sorted(list(set([entity_dict[e] for e in d if e.startswith(\"@entity\")]))) for d, _, _ in thing]\n",
        "      entities = [entities[i] for i in idx]\n",
        "      \n",
        "      predictions = output[0][entities[0]]\n",
        "      index_of_correct = entities[0].index(expected[0].item())\n",
        "      criterion = nn.CrossEntropyLoss()\n",
        "#       return criterion(predictions.unsqueeze(0), torch.Tensor([index_of_correct]).long().to(device))\n",
        "      \n",
        "      _, argmax = torch.max(predictions, 0)\n",
        "      print(argmax, len(predictions))\n",
        "      if (argmax.item() == index_of_correct):\n",
        "        correct += 1\n",
        "      total += 1\n",
        "  return correct / total\n",
        "      \n",
        "print(test_model(model, test_set))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kUbR9XgB7g7g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "! export CUDA_LAUNCH_BLOCKING=1; python3 foo.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "94wnHXKlj6pC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test_model(model, test_data):\n",
        "  total = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    test_loader = DataLoader(test_set, batch_size = 1, collate_fn=lambda x : x)\n",
        "    for i, thing in enumerate(test_loader):\n",
        "      sentence_converted = [(index_strings(d), index_strings(q), int(e[7:])) for d, q, e in thing]\n",
        "      dq = [torch.Tensor(d + [1] + q).long().to(device) for d, q, e in sentence_converted]\n",
        "      lengths = torch.Tensor([len(c) for c in dq])\n",
        "      lengths, idx = lengths.sort(0, descending=True)\n",
        "      dq = [dq[i] for i in idx]\n",
        "      expected = torch.Tensor([e for _, _, e in sentence_converted]).flatten().long().to(device)\n",
        "      expected = expected[idx]\n",
        "      output = model(dq)\n",
        "      \n",
        "      entities = [sorted(list(set([int(e[7:]) for e in d if e.startswith(\"@entity\")]))) for d, _, _ in thing]\n",
        "      entities = [entities[i] for i in idx]\n",
        "      \n",
        "      predictions = output[0][entities[0]]\n",
        "      index_of_correct = entities[0].index(expected[0].item())\n",
        "      criterion = nn.CrossEntropyLoss()\n",
        "#       return criterion(predictions.unsqueeze(0), torch.Tensor([index_of_correct]).long().to(device))\n",
        "      \n",
        "      _, argmax = torch.max(predictions, 0)\n",
        "      print(argmax, len(predictions))\n",
        "      if (argmax.item() == index_of_correct):\n",
        "        correct += 1\n",
        "      total += 1\n",
        "  return correct / total\n",
        "      \n",
        "test_model(model, test_set)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}